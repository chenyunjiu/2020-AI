{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理\n",
    "\n",
    "##### 文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：\n",
    "\n",
    "#####  1.读入文本\n",
    "#####  2.分词\n",
    "#####  3.建立字典，将每个词映射到一个唯一的索引（index）\n",
    "#####  4.将文本从词的序列转换为索引的序列，方便输入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 读入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def read_time_machine():\n",
    "    with open('/Users/CYJ/Desktop/timemachine2.txt', 'r') as f:\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append( re.sub('[^a-z]+', ' ',line.strip().lower()) )\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the time machine', '', '', 'an invention', '', '', '', '', 'by h g wells', '', '', '', '', 'contents', '', '', 'i introduction', 'ii the machine']\n",
      "# sentences 18\n"
     ]
    }
   ],
   "source": [
    "lines = read_time_machine()\n",
    "print(lines)\n",
    "print('# sentences %d' % len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences, token='word'):\n",
    "    'Split sentence into word or char tokens'\n",
    "    if token == 'word':\n",
    "        return [sentence.split(' ') for sentence in sentences]\n",
    "    elif token == 'char':\n",
    "        return [list(sentence) for sentence in sentences]\n",
    "    else:\n",
    "        print('Error: unknow token type' + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'time', 'machine'], [''], [''], ['an', 'invention'], [''], [''], [''], [''], ['by', 'h', 'g', 'wells'], [''], [''], [''], [''], ['contents'], [''], [''], ['i', 'introduction'], ['ii', 'the', 'machine']]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(lines)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 建立词典\n",
    "\n",
    "#### 先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def count_corpus(sentences):\n",
    "    tokens = [tk for st in sentences for tk in st]\n",
    "    return collections.Counter(tokens) #返回词典，记录每个词出现频数\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, tokens, min_freq=0, use_special_tokems=False):\n",
    "        counter = count_corpus(tokens)#每个词出现的频数，字典类型\n",
    "        self.token_freqs = list(counter.items())\n",
    "        \n",
    "        self.idx_to_token = [] #单词集合，无重复元素\n",
    "        if use_special_tokems: # ??\n",
    "            # padding, begin of sentence, end of sentence, unknown\n",
    "            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
    "            self.idx_to_token += ['', '', '', '']\n",
    "        else:\n",
    "            self.unk = 0\n",
    "            self.idx_to_token += ['']                \n",
    "        #去重 & 去低频\n",
    "        self.idx_to_token = [token for token, freq in self.token_freqs\n",
    "                          if freq > 0 and token not in self.idx_to_token]            \n",
    "        self.token_to_idx = dict() #创建唯一索引的词典      \n",
    "        for idx, token in enumerate(self.idx_to_token):\n",
    "            self.token_to_idx[token] = idx\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0), ('time', 1), ('machine', 2), ('an', 3), ('invention', 4), ('by', 5), ('h', 6), ('g', 7), ('wells', 8), ('contents', 9), ('i', 10), ('introduction', 11), ('ii', 12)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 将词转为索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['an', 'invention']\n",
      "indices: [3, 4]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['by', 'h', 'g', 'wells']\n",
      "indices: [5, 6, 7, 8]\n",
      "words: %s ['']\n",
      "indices: [0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print('words: ', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.语言模型数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63282\n",
      "爱女人\n",
      "透明的让我感动的可爱女人\n",
      "坏坏的\n",
      "想要有直升机 想要和\n"
     ]
    }
   ],
   "source": [
    "with open('../../datasets/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read()\n",
    "print(len(corpus_chars))\n",
    "print(corpus_chars[100: 120])\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[: 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 建立字符索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "98",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-953301e434cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m98\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 98"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars)) #去重\n",
    "\n",
    "char_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射\n",
    "vocab_size = len(char_to_idx)\n",
    "print(vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“MachineLearning”",
   "language": "python",
   "name": "machginelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
