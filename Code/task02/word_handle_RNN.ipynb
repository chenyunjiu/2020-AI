{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理\n",
    "\n",
    "##### 文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：\n",
    "\n",
    "#####  1.读入文本\n",
    "#####  2.分词\n",
    "#####  3.建立字典，将每个词映射到一个唯一的索引（index）\n",
    "#####  4.将文本从词的序列转换为索引的序列，方便输入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 读入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\")\n",
    "import d2l_jay9460 as d2l\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the time machine', '', '', 'an invention', '', '', '', '', 'by h g wells', '', '', '', '', 'contents', '', '', 'i introduction', 'ii the machine']\n",
      "# sentences 18\n"
     ]
    }
   ],
   "source": [
    "lines = read_time_machine()\n",
    "print(lines)\n",
    "print('# sentences %d' % len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences, token='word'):\n",
    "    'Split sentence into word or char tokens'\n",
    "    if token == 'word':\n",
    "        return [sentence.split(' ') for sentence in sentences]\n",
    "    elif token == 'char':\n",
    "        return [list(sentence) for sentence in sentences]\n",
    "    else:\n",
    "        print('Error: unknow token type' + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'time', 'machine'], [''], [''], ['an', 'invention'], [''], [''], [''], [''], ['by', 'h', 'g', 'wells'], [''], [''], [''], [''], ['contents'], [''], [''], ['i', 'introduction'], ['ii', 'the', 'machine']]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(lines)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 建立词典\n",
    "\n",
    "#### 先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def count_corpus(sentences):\n",
    "    tokens = [tk for st in sentences for tk in st]\n",
    "    return collections.Counter(tokens) #返回词典，记录每个词出现频数\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, tokens, min_freq=0, use_special_tokems=False):\n",
    "        counter = count_corpus(tokens)#每个词出现的频数，字典类型\n",
    "        self.token_freqs = list(counter.items())\n",
    "        \n",
    "        self.idx_to_token = [] #单词集合，无重复元素\n",
    "        if use_special_tokems: # ??\n",
    "            # padding, begin of sentence, end of sentence, unknown\n",
    "            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
    "            self.idx_to_token += ['', '', '', '']\n",
    "        else:\n",
    "            self.unk = 0\n",
    "            self.idx_to_token += ['']                \n",
    "        #去重 & 去低频\n",
    "        self.idx_to_token = [token for token, freq in self.token_freqs\n",
    "                          if freq > 0 and token not in self.idx_to_token]            \n",
    "        self.token_to_idx = dict() #创建唯一索引的词典      \n",
    "        for idx, token in enumerate(self.idx_to_token):\n",
    "            self.token_to_idx[token] = idx\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0), ('time', 1), ('machine', 2), ('an', 3), ('invention', 4), ('by', 5), ('h', 6), ('g', 7), ('wells', 8), ('contents', 9), ('i', 10), ('introduction', 11), ('ii', 12)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 将词转为索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['an', 'invention']\n",
      "indices: [3, 4]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['']\n",
      "indices: [0]\n",
      "words: %s ['by', 'h', 'g', 'wells']\n",
      "indices: [5, 6, 7, 8]\n",
      "words: %s ['']\n",
      "indices: [0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print('words: ', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.语言模型数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63282\n",
      "爱女人\n",
      "透明的让我感动的可爱女人\n",
      "坏坏的\n"
     ]
    }
   ],
   "source": [
    "with open('../../datasets/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read()\n",
    "print(len(corpus_chars))\n",
    "print(corpus_chars[100: 120])\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[: 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 建立字符索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n",
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [269, 17, 505, 106, 89, 131, 579, 269, 17, 745, 837, 143, 871, 51, 468, 973, 579, 269, 17, 745]\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars)) #去重\n",
    "\n",
    "char_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射\n",
    "vocab_size = len(char_to_idx)\n",
    "print(vocab_size)\n",
    "\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列\n",
    "sample = corpus_indices[: 20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n",
    "    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符\n",
    "    num_example = (len(corpus_indices) - 1) // num_steps # 下取整\n",
    "    # 每个样本的第一个字符在corpus_indices中的下标\n",
    "    example_indices = [i * num_steps for i in range(num_example)]\n",
    "    \n",
    "    #\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    def _data(i):\n",
    "        return corpus_indices[i:i+num_steps]\n",
    "    \n",
    "    for i in range(0, num_example, batch_size):\n",
    "        # python 生成器，返回迭代器\n",
    "        batch_indices = example_indices\n",
    "        X = [_data(j) for j in batch_indices]\n",
    "        Y = [_data(j+1) for j in batch_indices]\n",
    "        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2  相邻采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\n",
    "#print( d2l.load_data_jay_lyrics() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 one-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "torch.Size([2, 1027])\n"
     ]
    }
   ],
   "source": [
    "def one_hot(x, n_class, dtype=torch.float32):\n",
    "    # X shape: (batch), output shape: (batch, n_class)\n",
    "    x = x.long()\n",
    "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n",
    "    res.scatter_(1, x.view(-1, 1), 1)\n",
    "    return res\n",
    "\n",
    "x = torch.tensor([0,2])\n",
    "y = one_hot(x,vocab_size)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 初始化参数模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size = shape))\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "    \n",
    "    #隐藏层\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens)) \n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
    "    \n",
    "    #输出层\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs,requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return (torch.zeros((batch_size, num_hiddens)), )\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = one_hot(torch.tensor([[output[-1]]]), vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“MachineLearning”",
   "language": "python",
   "name": "machginelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
